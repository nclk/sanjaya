"""SQLAlchemy Core implementation of :class:`~sanjaya_core.provider.DataProvider`.

``SQLAlchemyProvider`` is configured with a SQLAlchemy :class:`~sqlalchemy.Table`
(or selectable), a list of :class:`~sanjaya_core.types.ColumnMeta` definitions,
and a :class:`~sqlalchemy.engine.Engine`.  It translates every provider
method into SQL using SQLAlchemy Core expressions — no ORM required.

Usage example::

    from sqlalchemy import MetaData, create_engine
    from sanjaya_core.types import ColumnMeta, DatasetCapabilities
    from sanjaya_core.enums import ColumnType
    from sanjaya_sqlalchemy import SQLAlchemyProvider

    engine = create_engine("postgresql://...")
    metadata = MetaData()
    metadata.reflect(bind=engine)
    trade_table = metadata.tables["trade_activity"]

    provider = SQLAlchemyProvider(
        key="trade_activity",
        label="Trade Activity",
        engine=engine,
        selectable=trade_table,
        columns=[
            ColumnMeta(name="id", label="ID", type=ColumnType.NUMBER),
            ColumnMeta(name="desk", label="Desk", type=ColumnType.STRING),
            ColumnMeta(name="amount", label="Amount", type=ColumnType.CURRENCY),
            ...
        ],
        capabilities=DatasetCapabilities(pivot=True),
    )

Both ``engine`` and ``selectable`` accept callables for fully deferred setup.
When ``selectable`` is a callable it receives the materialised engine, which
makes ``autoload_with=engine`` possible without needing the engine at import
time.  ``columns`` may be omitted entirely — they will be inferred from the
resolved selectable::

    provider = SQLAlchemyProvider(
        key="trade_activity",
        label="Trade Activity",
        engine=lambda: create_engine("postgresql://..."),
        selectable=lambda engine: sa.Table(
            "trade_activity", sa.MetaData(), autoload_with=engine,
        ),
        # columns omitted → auto-inferred from reflected table
    )

Nothing touches the database until the first query or ``get_columns()`` call.
"""

from __future__ import annotations

import logging
from typing import Any, Callable

import sqlalchemy as sa
from sqlalchemy import FromClause
from sqlalchemy.engine import Engine
from sqlalchemy.sql.expression import ColumnElement, SelectBase

logger = logging.getLogger(__name__)

from sanjaya_core.context import RequestContext
from sanjaya_core.enums import AggFunc, ColumnType, SortDirection
from sanjaya_core.filters import FilterGroup
from sanjaya_core.provider import DataProvider
from sanjaya_core.types import (
    AggregateColumn,
    AggregateResult,
    ColumnMeta,
    DatasetCapabilities,
    SortSpec,
    TabularResult,
    ValueSpec,
)

from sanjaya_sqlalchemy.filters import compile_filter_group


#: Mapping from SQLAlchemy column types to :class:`ColumnType`.
#: Used by :func:`infer_column_type` when ``columns`` is omitted.
_SA_TYPE_MAP: list[tuple[type[sa.types.TypeEngine], ColumnType]] = [
    (sa.types.Boolean, ColumnType.BOOLEAN),
    (sa.types.Date, ColumnType.DATE),
    (sa.types.DateTime, ColumnType.DATETIME),
    (sa.types.Float, ColumnType.NUMBER),
    (sa.types.Integer, ColumnType.NUMBER),
    (sa.types.Numeric, ColumnType.NUMBER),
    (sa.types.String, ColumnType.STRING),
    (sa.types.Text, ColumnType.STRING),
]


def infer_column_type(sa_type: sa.types.TypeEngine) -> ColumnType:
    """Map a SQLAlchemy column type to a :class:`ColumnType`.

    Falls back to :attr:`ColumnType.STRING` for unknown types.
    """
    for sa_cls, col_type in _SA_TYPE_MAP:
        if isinstance(sa_type, sa_cls):
            return col_type
    return ColumnType.STRING


def columns_from_selectable(selectable: FromClause) -> list[ColumnMeta]:
    """Derive :class:`ColumnMeta` from the columns of a SQLAlchemy selectable.

    Column labels are generated by title-casing the column name with
    underscores replaced by spaces (e.g. ``trade_date`` → ``Trade Date``).
    """
    result: list[ColumnMeta] = []
    for col in selectable.columns:
        col_type = infer_column_type(col.type)
        # ColumnClause (produced by subqueries, sa.table(), sa.column(), etc.)
        # does not carry a .nullable attribute — only schema-bound Column
        # does.  Default to True (assume nullable) when the attribute is
        # absent, which is the safer assumption for query generation.
        nullable = getattr(col, "nullable", None)
        result.append(
            ColumnMeta(
                name=col.name,
                label=col.name.replace("_", " ").title(),
                type=col_type,
                nullable=nullable if nullable is not None else True,
            )
        )
    return result


class SQLAlchemyProvider(DataProvider):
    """A :class:`DataProvider` backed by a SQLAlchemy selectable + engine.

    All three data-bearing parameters — ``engine``, ``selectable``, and
    ``columns`` — support deferred resolution so that nothing touches the
    database until the first actual query or ``get_columns()`` call.

    Parameters
    ----------
    key:
        Dataset identifier.
    label:
        Human-readable name.
    engine:
        A SQLAlchemy :class:`~sqlalchemy.engine.Engine` used to execute
        queries, **or** a zero-argument callable that returns one.  When
        a callable is provided the engine is created lazily on the first
        query, which avoids expensive connection setup at import time.
    selectable:
        The table, subquery, or select statement to query against
        (e.g. a :class:`~sqlalchemy.Table`, ``select()``, or any
        :class:`~sqlalchemy.sql.expression.SelectBase`).  A
        ``SelectBase`` is automatically wrapped via ``.subquery()``.

        May also be a **callable** that receives the materialised
        ``Engine`` and returns a ``FromClause`` or ``SelectBase``.
        This is the recommended pattern when the table is reflected via
        ``autoload_with=engine``.
    columns:
        Column metadata definitions.  Each
        :attr:`~sanjaya_core.types.ColumnMeta.name` must match a column
        name in *selectable*.

        When *None* (the default), columns are **auto-inferred** from
        the resolved selectable using :func:`columns_from_selectable`.
        Pass an explicit list when you need custom labels, pivot options,
        format hints, etc.
    description:
        Optional description.
    capabilities:
        Optional capability flags.
    """

    def __init__(
        self,
        *,
        key: str,
        label: str,
        engine: Engine | Callable[[], Engine],
        selectable: FromClause | SelectBase | Callable[[Engine], FromClause | SelectBase],
        columns: list[ColumnMeta] | None = None,
        description: str = "",
        capabilities: DatasetCapabilities | None = None,
    ) -> None:
        super().__init__(
            key=key,
            label=label,
            description=description,
            capabilities=capabilities or DatasetCapabilities(pivot=True),
        )

        # --- engine (eager or lazy) --------------------------------
        if callable(engine) and not isinstance(engine, Engine):
            self._engine_factory: Callable[[], Engine] | None = engine
            self._engine_instance: Engine | None = None
        else:
            self._engine_factory = None
            self._engine_instance = engine  # type: ignore[assignment]

        # --- selectable / columns (may require deferred init) ------
        self._initialized = False
        self._selectable_input = selectable
        self._columns_input = columns
        # These are populated by _ensure_initialized().
        self._selectable: FromClause | None = None  # type: ignore[assignment]
        self._columns: list[ColumnMeta] | None = None
        self._column_lookup: dict[str, ColumnElement[Any]] = {}

        # Eagerly initialize when possible (no callables involved).
        if not callable(selectable) or isinstance(selectable, (FromClause, SelectBase)):
            self._materialize_selectable_and_columns(selectable, columns)  # type: ignore[arg-type]

    # ------------------------------------------------------------------
    # Deferred initialization
    # ------------------------------------------------------------------

    @property
    def _engine(self) -> Engine:
        """Return the engine, creating it on first access if needed."""
        if self._engine_instance is None:
            assert self._engine_factory is not None
            self._engine_instance = self._engine_factory()
            self._engine_factory = None  # release the factory reference
        return self._engine_instance

    def _materialize_selectable_and_columns(
        self,
        raw_selectable: FromClause | SelectBase,
        raw_columns: list[ColumnMeta] | None,
    ) -> None:
        """Resolve selectable and columns from their raw inputs."""
        self._selectable = (
            raw_selectable.subquery()
            if isinstance(raw_selectable, SelectBase)
            else raw_selectable
        )
        self._columns = (
            raw_columns
            if raw_columns is not None
            else columns_from_selectable(self._selectable)
        )
        self._column_lookup = {
            c.name: self._selectable.c[c.name] for c in self._columns
        }
        self._initialized = True

    def _ensure_initialized(self) -> None:
        """Materialize engine → selectable → columns on first use."""
        if self._initialized:
            return

        inp = self._selectable_input
        if callable(inp) and not isinstance(inp, (FromClause, SelectBase)):
            logger.debug("Resolving deferred selectable for %r", self.key)
            raw_selectable = inp(self._engine)
        else:
            raw_selectable = inp  # type: ignore[assignment]

        self._materialize_selectable_and_columns(
            raw_selectable, self._columns_input,
        )
        # Release references to the raw inputs.
        self._selectable_input = None  # type: ignore[assignment]
        self._columns_input = None

    @property
    def _resolved_selectable(self) -> FromClause:
        """Return the selectable, asserting it has been initialized."""
        assert self._selectable is not None, (
            "_selectable is None — call _ensure_initialized() first"
        )
        return self._selectable

    # ------------------------------------------------------------------
    # DataProvider interface
    # ------------------------------------------------------------------

    def get_columns(self) -> list[ColumnMeta]:
        self._ensure_initialized()
        assert self._columns is not None
        return list(self._columns)

    def query(
        self,
        selected_columns: list[str],
        *,
        filter_group: FilterGroup | None = None,
        sort: list[SortSpec] | None = None,
        limit: int = 100,
        offset: int = 0,
        ctx: RequestContext | None = None,
    ) -> TabularResult:
        self._ensure_initialized()
        # --- count ---
        selectable = self._resolved_selectable
        count_stmt = sa.select(sa.func.count()).select_from(selectable)
        if filter_group:
            count_stmt = count_stmt.where(
                compile_filter_group(filter_group, self._column_lookup)
            )

        # --- data ---
        cols = [self._column_lookup[c] for c in selected_columns]
        data_stmt = sa.select(*cols).select_from(selectable)
        if filter_group:
            data_stmt = data_stmt.where(
                compile_filter_group(filter_group, self._column_lookup)
            )
        data_stmt = self._apply_sort(data_stmt, sort, fallback_columns=cols)
        data_stmt = data_stmt.limit(limit)
        if offset:
            data_stmt = data_stmt.offset(offset)

        with self._engine.connect() as conn:
            total = conn.execute(count_stmt).scalar_one()
            rows = [dict(r._mapping) for r in conn.execute(data_stmt)]

        return TabularResult(columns=selected_columns, rows=rows, total=total)

    def aggregate(
        self,
        group_by_rows: list[str],
        group_by_cols: list[str],
        values: list[ValueSpec],
        *,
        filter_group: FilterGroup | None = None,
        sort: list[SortSpec] | None = None,
        limit: int | None = None,
        offset: int = 0,
        ctx: RequestContext | None = None,
    ) -> AggregateResult:
        self._ensure_initialized()
        if group_by_cols:
            return self._pivot_aggregate(
                group_by_rows, group_by_cols, values,
                filter_group=filter_group, sort=sort,
                limit=limit, offset=offset,
            )
        return self._simple_aggregate(
            group_by_rows, values,
            filter_group=filter_group, sort=sort,
            limit=limit, offset=offset,
        )

    # ------------------------------------------------------------------
    # Internal: simple (non-pivot) aggregation
    # ------------------------------------------------------------------

    def _simple_aggregate(
        self,
        group_by_rows: list[str],
        values: list[ValueSpec],
        *,
        filter_group: FilterGroup | None = None,
        sort: list[SortSpec] | None = None,
        limit: int | None = None,
        offset: int = 0,
    ) -> AggregateResult:
        # Build SELECT clause: group-by columns + aggregate expressions.
        group_cols = [self._column_lookup[c] for c in group_by_rows]
        agg_exprs: list[tuple[str, ColumnElement[Any]]] = []
        result_columns: list[AggregateColumn] = []

        for col_name in group_by_rows:
            result_columns.append(
                AggregateColumn(key=col_name, header=col_name)
            )

        for vs in values:
            key = f"{vs.agg}_{vs.column}"
            sa_expr = self._agg_expression(vs)
            agg_exprs.append((key, sa_expr))
            result_columns.append(
                AggregateColumn(
                    key=key,
                    header=vs.label or key,
                    measure=vs.column,
                    agg=vs.agg,
                )
            )

        select_cols = [
            *[c.label(c.name) for c in group_cols],
            *[expr.label(key) for key, expr in agg_exprs],
        ]

        # --- count (total groups) ---
        selectable = self._resolved_selectable
        count_sub = (
            sa.select(*[c.label(c.name) for c in group_cols])
            .select_from(selectable)
            .group_by(*group_cols)
        )
        if filter_group:
            count_sub = count_sub.where(
                compile_filter_group(filter_group, self._column_lookup)
            )
        count_stmt = sa.select(sa.func.count()).select_from(count_sub.subquery())

        # --- data ---
        data_stmt = (
            sa.select(*select_cols)
            .select_from(selectable)
            .group_by(*group_cols)
        )
        if filter_group:
            data_stmt = data_stmt.where(
                compile_filter_group(filter_group, self._column_lookup)
            )
        data_stmt = self._apply_sort(data_stmt, sort, fallback_columns=group_cols)
        if limit is not None:
            data_stmt = data_stmt.limit(limit)
        if offset:
            data_stmt = data_stmt.offset(offset)

        with self._engine.connect() as conn:
            total = conn.execute(count_stmt).scalar_one()
            rows = [dict(r._mapping) for r in conn.execute(data_stmt)]

        return AggregateResult(columns=result_columns, rows=rows, total=total)

    # ------------------------------------------------------------------
    # Internal: pivot aggregation
    # ------------------------------------------------------------------

    def _pivot_aggregate(
        self,
        group_by_rows: list[str],
        group_by_cols: list[str],
        values: list[ValueSpec],
        *,
        filter_group: FilterGroup | None = None,
        sort: list[SortSpec] | None = None,
        limit: int | None = None,
        offset: int = 0,
    ) -> AggregateResult:
        """Two-pass pivot: discover combos, then aggregate per combo.

        SQLAlchemy Core doesn't have built-in PIVOT support, so we:
        1. Query distinct pivot-column combinations.
        2. Build one ``CASE WHEN … END`` per (combo × measure) to simulate
           a pivot in a single grouped query.
        """
        where_clause: ColumnElement[bool] | None = None
        if filter_group:
            where_clause = compile_filter_group(
                filter_group, self._column_lookup
            )

        # --- pass 1: discover distinct pivot combos ---
        selectable = self._resolved_selectable
        pivot_sa_cols = [self._column_lookup[c] for c in group_by_cols]
        combo_stmt = (
            sa.select(*pivot_sa_cols)
            .select_from(selectable)
            .distinct()
        )
        if where_clause is not None:
            combo_stmt = combo_stmt.where(where_clause)
        # Deterministic ordering of combos.
        combo_stmt = combo_stmt.order_by(*pivot_sa_cols)

        with self._engine.connect() as conn:
            combos = [tuple(r._mapping[c] for c in group_by_cols) for r in conn.execute(combo_stmt)]

        # --- build result column metadata + CASE expressions ---
        group_row_cols = [self._column_lookup[c] for c in group_by_rows]

        result_columns: list[AggregateColumn] = []
        for col_name in group_by_rows:
            result_columns.append(
                AggregateColumn(key=col_name, header=col_name)
            )

        case_exprs: list[tuple[str, ColumnElement[Any]]] = []

        for combo in combos:
            # Build the CASE WHEN condition for this combo.
            combo_cond = sa.and_(
                *(
                    self._column_lookup[group_by_cols[i]] == combo[i]
                    for i in range(len(group_by_cols))
                )
            )
            for vs in values:
                pivot_key_parts = [str(v) for v in combo]
                col_key = "_".join(pivot_key_parts + [vs.agg, vs.column])
                result_columns.append(
                    AggregateColumn(
                        key=col_key,
                        header=col_key,
                        pivot_keys=pivot_key_parts,
                        measure=vs.column,
                        agg=vs.agg,
                    )
                )
                # CASE WHEN <combo_cond> THEN <measure_col> END → wrapped in agg
                measure_col = self._column_lookup[vs.column]
                case_expr = sa.case((combo_cond, measure_col))
                agg_expr = self._wrap_agg(vs.agg, case_expr)
                case_exprs.append((col_key, agg_expr))

        # --- pass 2: grouped query with CASE expressions ---
        select_cols = [
            *[c.label(c.name) for c in group_row_cols],
            *[expr.label(key) for key, expr in case_exprs],
        ]

        # count total groups
        count_sub = (
            sa.select(*[c.label(c.name) for c in group_row_cols])
            .select_from(selectable)
            .group_by(*group_row_cols)
        )
        if where_clause is not None:
            count_sub = count_sub.where(where_clause)
        count_stmt = sa.select(sa.func.count()).select_from(count_sub.subquery())

        data_stmt = (
            sa.select(*select_cols)
            .select_from(selectable)
            .group_by(*group_row_cols)
        )
        if where_clause is not None:
            data_stmt = data_stmt.where(where_clause)
        data_stmt = self._apply_sort(data_stmt, sort, fallback_columns=group_row_cols)
        if limit is not None:
            data_stmt = data_stmt.limit(limit)
        if offset:
            data_stmt = data_stmt.offset(offset)

        with self._engine.connect() as conn:
            total = conn.execute(count_stmt).scalar_one()
            rows = [dict(r._mapping) for r in conn.execute(data_stmt)]

        return AggregateResult(columns=result_columns, rows=rows, total=total)

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------

    def _apply_sort(
        self,
        stmt: sa.Select[Any],
        sort: list[SortSpec] | None,
        fallback_columns: list[ColumnElement[Any]] | None = None,
    ) -> sa.Select[Any]:
        """Append ORDER BY clauses to *stmt*.

        Parameters
        ----------
        stmt:
            The select statement to augment.
        sort:
            Explicit sort directives from the caller.
        fallback_columns:
            Columns to use as a deterministic fallback when *sort* is
            empty.  MSSQL / Azure SQL requires an ``ORDER BY`` whenever
            ``OFFSET`` or a non-simple ``LIMIT`` clause is present;
            providing a fallback avoids the
            ``MSSQL requires an order_by when using an OFFSET or a
            non-simple LIMIT clause`` error.
        """
        if sort:
            clauses = []
            for spec in sort:
                col = self._column_lookup[spec.column]
                clauses.append(
                    col.desc() if spec.direction == SortDirection.DESC else col.asc()
                )
            return stmt.order_by(*clauses)

        if fallback_columns:
            return stmt.order_by(*[c.asc() for c in fallback_columns])

        # Ultimate fallback: MSSQL requires ORDER BY whenever OFFSET is
        # present.  ``ORDER BY 1`` (first column in the select list) is
        # the cheapest deterministic ordering we can guarantee.
        return stmt.order_by(sa.literal_column("1"))

    def _agg_expression(self, vs: ValueSpec) -> ColumnElement[Any]:
        """Build a SQLAlchemy aggregate expression for a :class:`ValueSpec`."""
        col = self._column_lookup[vs.column]
        return self._wrap_agg(vs.agg, col)

    @staticmethod
    def _wrap_agg(agg: AggFunc, expr: ColumnElement[Any]) -> ColumnElement[Any]:
        """Wrap *expr* in the SQL aggregate function for *agg*."""
        match agg:
            case AggFunc.SUM:
                return sa.func.sum(expr)
            case AggFunc.AVG:
                return sa.func.avg(expr)
            case AggFunc.MIN:
                return sa.func.min(expr)
            case AggFunc.MAX:
                return sa.func.max(expr)
            case AggFunc.COUNT:
                return sa.func.count(expr)
            case AggFunc.DISTINCT_COUNT:
                return sa.func.count(sa.distinct(expr))
            case AggFunc.FIRST:
                return sa.func.min(expr)  # approximation — no SQL FIRST
            case AggFunc.LAST:
                return sa.func.max(expr)  # approximation — no SQL LAST
            case _:
                return sa.func.count(expr)
